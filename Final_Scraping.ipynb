{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scraping the search page "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When logging this process we use the class connector from the scraping_class. But to use it for our purpose, there had to be made som changes.\n",
    "\n",
    "The changes:\n",
    "\n",
    "1) Change Connector_type to 'selenium'\n",
    "\n",
    "2) Insert path2selenium\n",
    "\n",
    "3) Insert code to scroll down on page to load more html content\n",
    "\n",
    "4) Return the connector.browser.page_source\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,os,time\n",
    "\n",
    "def ratelimit(dt):\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(dt) # sleep one second.\n",
    "\n",
    "class Connector():\n",
    "  def __init__(self,logfile,overwrite_log=False,connector_type='selenium',session=False,path2selenium='/Users/user/Downloads/chromedriver',n_tries = 5,timeout=30,waiting_time=0.5):\n",
    "    \"\"\"This Class implements a method for reliable connection to the internet and monitoring. \n",
    "    It handles simple errors due to connection problems, and logs a range of information for basic quality assessments\n",
    "    \n",
    "    Keyword arguments:\n",
    "    logfile -- path to the logfile\n",
    "    overwrite_log -- bool, defining if logfile should be cleared (rarely the case). \n",
    "    connector_type -- use the 'requests' module or the 'selenium'. Will have different since the selenium webdriver does not have a similar response object when using the get method, and monitoring the behavior cannot be automated in the same way.\n",
    "    session -- requests.session object. For defining custom headers and proxies.\n",
    "    path2selenium -- str, sets the path to the geckodriver needed when using selenium.\n",
    "    n_tries -- int, defines the number of retries the *get* method will try to avoid random connection errors.\n",
    "    timeout -- int, seconds the get request will wait for the server to respond, again to avoid connection errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialization function defining parameters. \n",
    "    self.n_tries = n_tries # For avoiding triviel error e.g. connection errors, this defines how many times it will retry.\n",
    "    self.timeout = timeout # Defining the maximum time to wait for a server to response.\n",
    "    self.waiting_time = waiting_time # define simple rate_limit parameter.\n",
    "    ## not implemented here, if you use selenium.\n",
    "    if connector_type=='selenium':\n",
    "      assert path2selenium!='', \"You need to specify the path to you geckodriver if you want to use Selenium\"\n",
    "      from selenium import webdriver \n",
    "      ## HIN download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "      assert os.path.isfile(path2selenium),'You need to insert a valid path2selenium the path to your geckodriver. You can download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases'\n",
    "      self.browser = webdriver.Chrome(executable_path=path2selenium) # start the browser with a path to the geckodriver.\n",
    "\n",
    "    self.connector_type = connector_type # set the connector_type\n",
    "    \n",
    "    if session: # set the custom session\n",
    "      self.session = session\n",
    "    else:\n",
    "      self.session = requests.session()\n",
    "    self.logfilename = logfile # set the logfile path\n",
    "    ## define header for the logfile\n",
    "    header = ['id','project','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "    if os.path.isfile(logfile):        \n",
    "      if overwrite_log==True:\n",
    "        self.log = open(logfile,'w')\n",
    "        self.log.write(';'.join(header))\n",
    "      else:\n",
    "        self.log = open(logfile,'a')\n",
    "    else:\n",
    "      self.log = open(logfile,'w')\n",
    "      self.log.write(';'.join(header))\n",
    "    ## load log \n",
    "    with open(logfile,'r') as f: # open file\n",
    "        \n",
    "      l = f.read().split('\\n') # read and split file by newlines.\n",
    "      ## set id\n",
    "      if len(l)<=1:\n",
    "        self.id = 0\n",
    "      else:\n",
    "        self.id = int(l[-1][0])+1\n",
    "            \n",
    "  def get(self,url,project_name):\n",
    "    \"\"\"Method for connector reliably to the internet, with multiple tries and simple error handling, as well as default logging function.\n",
    "    Input url and the project name for the log (i.e. is it part of mapping the domain, or is it the part of the final stage in the data collection).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- str, url\n",
    "    project_name -- str, fName used for analyzing the log. Use case could be the 'Mapping of domain','Meta_data_collection','main data collection'. \n",
    "    \"\"\"\n",
    "     \n",
    "    project_name = project_name.replace(';','-') # make sure the default csv seperator is not in the project_name.\n",
    "    if self.connector_type=='requests': # Determine connector method.\n",
    "      for _ in range(self.n_tries): # for loop defining number of retries with the requests method.\n",
    "        ratelimit(self.waiting_time)\n",
    "        t = time.time()\n",
    "        try: # error handling \n",
    "          response = self.session.get(url,timeout = self.timeout) # make get call\n",
    "\n",
    "          err = '' # define python error variable as empty assumming success.\n",
    "          success = True # define success variable\n",
    "          redirect_url = response.url # log current url, after potential redirects \n",
    "          dt = t - time.time() # define delta-time waiting for the server and downloading content.\n",
    "          size = len(response.text) # define variable for size of html content of the response.\n",
    "          response_code = response.status_code # log status code.\n",
    "          ## log...\n",
    "          call_id = self.id # get current unique identifier for the call\n",
    "          self.id+=1 # increment call id\n",
    "          #['id','project_name','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row to be written in the log.\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write log.\n",
    "          self.log.flush()\n",
    "          return response,call_id # return response and unique identifier.\n",
    "\n",
    "        except Exception as e: # define error condition\n",
    "          err = str(e) # python error\n",
    "          response_code = '' # blank response code \n",
    "          success = False # call success = False\n",
    "          size = 0 # content is empty.\n",
    "          redirect_url = '' # redirect url empty \n",
    "          dt = t - time.time() # define delta t\n",
    "\n",
    "          ## log...\n",
    "          call_id = self.id # define unique identifier\n",
    "          self.id+=1 # increment call_id\n",
    "\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write row to log.\n",
    "          self.log.flush()\n",
    "    else:\n",
    "      t = time.time()\n",
    "      ratelimit(self.waiting_time)\n",
    "      self.browser.get(url) # use selenium get method\n",
    "      \n",
    "      # Slowly scrolling down the page, in order to load more \n",
    "      y = 1000\n",
    "      for timer in range(0,2000):\n",
    "          self.browser.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "          y += 1000  \n",
    "          time.sleep(3)\n",
    "        \n",
    "      ## log\n",
    "      call_id = self.id # define unique identifier for the call. \n",
    "      self.id+=1 # increment the call_id\n",
    "      err = '' # blank error message\n",
    "      success = '' # success blank\n",
    "      redirect_url = self.browser.current_url # redirect url.\n",
    "      dt = t - time.time() # get time for get method ... NOTE: not necessarily the complete load time.\n",
    "      size = len(self.browser.page_source) # get size of content ... NOTE: not necessarily correct, since selenium works in the background, and could still be loading.\n",
    "      response_code = '' # empty response code.\n",
    "      row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row \n",
    "      self.log.write('\\n'+';'.join(map(str,row))) # write row to log file.\n",
    "      self.log.flush()\n",
    "    # Using selenium it will not return a response object, instead you should call the browser object of the connector.\n",
    "      html=connector.browser.page_source\n",
    "      return html,call_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium request\n",
    "\n",
    "The Vivino page has a search page, with opportunity to filter on different variables. We chose to filter on red wines from Italy and the price was set to be 0-2500+. This gave us 99 841 wines, and we were far from collecting information on them all, since scrolling usually stopped after 3000 wines, and it was time consuming. \n",
    "The page had to either be sorted by ratings, price, discount or popularity.\n",
    "So we did it over several links, where we changed the way they were sorted by, to get wines from various price and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define logfile \n",
    "logfile = 'logfile_wine.csv' ## name your log file.\n",
    "connector = Connector(logfile)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Insert a URL of the search page\n",
    "url = 'https://www.vivino.com/explore?e=eJwNijEOgCAQBH-zNZpYXmfHE4wxJyK5RMDABfX30swUM7HQgCiJDCK_NE7GwH00W7gOi7vncFLjIl75Qt6psEoKdePmCwePTIevDo8ua5_dV7tFfwGHHeQ='\n",
    "\n",
    "# Connector class returns html and call_id\n",
    "html, call_id =connector.get(url,\"Searchpage\")\n",
    "\n",
    "# parse the html to a soup\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(\"--- %s seconds ---\" % round((time.time() - start_time),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting from soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>region</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toscana 2008</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Toscana</td>\n",
       "      <td>https://www.vivino.com/masseto-toscana/w/24467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toscana 2006</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Toscana</td>\n",
       "      <td>https://www.vivino.com/masseto-toscana/w/24467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toscana 2015</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Toscana</td>\n",
       "      <td>https://www.vivino.com/masseto-toscana/w/24467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toscana 2009</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Toscana</td>\n",
       "      <td>https://www.vivino.com/masseto-toscana/w/24467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toscana 2007</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Toscana</td>\n",
       "      <td>https://www.vivino.com/masseto-toscana/w/24467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>Farnito Cabernet Sauvignon Toscana 2010</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Toscana</td>\n",
       "      <td>https://www.vivino.com/carpineto-farnito-caber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>Morellino di Scansano 2017</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Morellino di Scansano</td>\n",
       "      <td>https://www.vivino.com/fattoria-le-pupille-mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>Barbera d'Alba 2016</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Barbera d'Alba</td>\n",
       "      <td>https://www.vivino.com/pio-cesare-barbera-d-al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>Bricco Ambrogio Barolo 2014</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Barolo</td>\n",
       "      <td>https://www.vivino.com/lodali-barolo-bricco-am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>Vigneto Pozzare Cabernet 2016</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Colli Berici</td>\n",
       "      <td>https://www.vivino.com/piovene-porto-godi-vign...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1937 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name rating                 region  \\\n",
       "0                                Toscana 2008    4.7                Toscana   \n",
       "1                                Toscana 2006    4.8                Toscana   \n",
       "2                                Toscana 2015    4.7                Toscana   \n",
       "3                                Toscana 2009    4.7                Toscana   \n",
       "4                                Toscana 2007    4.7                Toscana   \n",
       "...                                       ...    ...                    ...   \n",
       "1932  Farnito Cabernet Sauvignon Toscana 2010    4.0                Toscana   \n",
       "1933               Morellino di Scansano 2017    3.6  Morellino di Scansano   \n",
       "1934                      Barbera d'Alba 2016    3.8         Barbera d'Alba   \n",
       "1935              Bricco Ambrogio Barolo 2014    4.2                 Barolo   \n",
       "1936            Vigneto Pozzare Cabernet 2016    3.8           Colli Berici   \n",
       "\n",
       "                                                   link  \n",
       "0     https://www.vivino.com/masseto-toscana/w/24467...  \n",
       "1     https://www.vivino.com/masseto-toscana/w/24467...  \n",
       "2     https://www.vivino.com/masseto-toscana/w/24467...  \n",
       "3     https://www.vivino.com/masseto-toscana/w/24467...  \n",
       "4     https://www.vivino.com/masseto-toscana/w/24467...  \n",
       "...                                                 ...  \n",
       "1932  https://www.vivino.com/carpineto-farnito-caber...  \n",
       "1933  https://www.vivino.com/fattoria-le-pupille-mor...  \n",
       "1934  https://www.vivino.com/pio-cesare-barbera-d-al...  \n",
       "1935  https://www.vivino.com/lodali-barolo-bricco-am...  \n",
       "1936  https://www.vivino.com/piovene-porto-godi-vign...  \n",
       "\n",
       "[1937 rows x 4 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Finding the needed information in the soup and makes a dataframe\n",
    "\n",
    "soup_select = soup.find_all('div', {'class': 'explorerCard__explorerCard--3Q7_0 explorerPageResults__explorerCard--3q6Qe'})\n",
    "vivino='https://www.vivino.com'\n",
    "urls=[]\n",
    "names=[]\n",
    "ratings=[]\n",
    "region=[]\n",
    "\n",
    "for i in soup_select:\n",
    "    # find all a tags -connoting a hyperlink.\n",
    "    links = i.find_all('a', {'class': 'anchor__anchor--2QZvA'}) \n",
    "    #  Save the links constructed in the urls list\n",
    "    urls.append([vivino+link['href'] for link in links if link.has_attr('href')][0])\n",
    "    # Saving wine titles in names\n",
    "    names.append(i.find_all('span', {'class': 'vintageTitle__wine--U7t9G'})[0].text)\n",
    "    # Saving total rating in ratings\n",
    "    ratings.append(i.find_all('div', {'class': 'vivinoRatingWide__averageValue--1zL_5'})[0].text)\n",
    "    # Saving region of the wine\n",
    "    region.append(i.find_all('a', {'class': 'anchor__anchor--2QZvA vintageLocation__anchor--T7J3k'})[2].text)\n",
    "\n",
    "'''The list are zipped and saved to a dataframe for later use. We applied drop_duplicates by link\n",
    "to avoid having to have two of the same wines in our dataset. The dataframe is then saved if needed.'''\n",
    "\n",
    "df=pd.DataFrame(zip(names,ratings,region,urls),columns=['name','rating','region','link'])\n",
    "vivino=df.drop_duplicates().reset_index(drop=True)\n",
    "#vivino.to_csv('vivino_random.csv')\n",
    "\n",
    "vivino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: if two dataframes have to be merged to create a big dataframe\n",
    "\n",
    "#Load the other dataframe/dataframes\n",
    "df_loaded=pd.read_csv('Insert file name.csv', index_col=[0])\n",
    "\n",
    "# Merging two dataframes\n",
    "merged=pd.concat([df_loaded,vivino])\n",
    "\n",
    "#drop duplicates and reset index \n",
    "df3=merged.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save the merged dataframe \n",
    "df3.to_csv('Insert filename of your choice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Scraping each link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data with selenium\n",
    "In this part we need to call different libraries, most importantly selenium and random_user_agent.\n",
    "\n",
    "We started a class called request in which the function get_selenium_res is defined. The function utilizes the random_user_agent module imported to automatically procure a random user agent for each selenium call we make. From there on we altered some chrome options, such as to go ”— headless”, which means that the chrome browser won’t physically open on our machines, the reason for that is to reduce the load on the machine’s cpu and to be able to loop over more  than one URL. Another option which had to be altered is “— window-size”; The window size must be stretched, since selenium only will return the domain that it was required to load.  \n",
    "   When opening a browser, we use WebDriverWait with a time frame of 90 seconds in which chrome will not close or return the html before a particular element is rendered. The element we chose has the specified particular class 'indicatorBar__progress--3aXLX', this will give us the information on the different taste profiles, later on the process, we discovered there were a few pages without this class name, so it had to be changed to 'vivinoRatingWide__averageValue--1zL_5'. The function will return the html of the page if succeeded, if not either a Timeout Exception or a WebDriverException will occur. In case of an exception; We return the same get_selenium_res function, since we would like to automatically retry. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException,ElementNotVisibleException\n",
    "from random_user_agent.user_agent import UserAgent \n",
    "from random_user_agent.params import SoftwareName, OperatingSystem \n",
    "from time import sleep\n",
    "import logging\n",
    "import traceback\n",
    "from self import self\n",
    " \n",
    "class Request:\n",
    "    \n",
    "    # These are used to keep track of progress in the log \n",
    "    selenium_retries=0\n",
    "    nr=1\n",
    "    \n",
    "    # Configuring a new file to save the log\n",
    "    logging.basicConfig(filename=\"TheScraping.log\",\n",
    "                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                        filemode='w')\n",
    "    #Creating af logger object, in order to reference to the same logger object\n",
    "    logger=logging.getLogger('wine_project.requests')\n",
    "    \n",
    "    # Setting the logging level to info\n",
    "    logger.setLevel(logging.INFO) \n",
    "  \n",
    "    def __init__(self,url):\n",
    "        self.url = url\n",
    "   \n",
    "    def get_selenium_res(self,class_name):\n",
    "        # Keeping track of how many requests has been made to the same URL\n",
    "        self.logger.info('sending request #: '+str(self.nr)+' to '+str(self.url))\n",
    "        self.nr+=1\n",
    "        \n",
    "        try:\n",
    "            # This part is to generate various user agents\n",
    "            software_names=[SoftwareName.CHROME.value]\n",
    "            operating_systems=[OperatingSystem.WINDOWS.value,\n",
    "                                  OperatingSystem.LINUX.value]\n",
    "            user_agent_rotator=UserAgent(software_names=software_names,\n",
    "                                            operating_systems=operating_systems,\n",
    "                                            limit=100)\n",
    "            user_agent=user_agent_rotator.get_random_user_agent()\n",
    "             \n",
    "            # Options for chrome is defines    \n",
    "            chrome_options=Options()\n",
    "            chrome_options.add_argument(\"--headless\") # Chrome won't open a browser everytime\n",
    "            chrome_options.add_argument(\"--no-sandbox\") # The only way to get chrome to open headlessly  \n",
    "            chrome_options.add_argument(\"--window-size=1420,1080\") # Stretching the window size\n",
    "            chrome_options.add_argument(f'user-agent={user_agent}') # Inserting a random user-agent \n",
    "            \n",
    "            # Setting driver with the specific options and pathway to the chromedriver\n",
    "            # nb: Change the pathway to chromedriver \n",
    "    \n",
    "            browser = webdriver.Chrome('/Users/lilly/Desktop/chromedriver',options=chrome_options)\n",
    "            \n",
    "            # Sending request for the html\n",
    "            browser.get(self.url)\n",
    "             \n",
    "            '''Webdriverwait sets a time frame for the page to load, chrome won't return the dom \n",
    "                until class_name variable is retrieved'''\n",
    "            time_to_wait=90    \n",
    "            WebDriverWait(browser,time_to_wait).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, class_name)))\n",
    "            \n",
    "        '''If selenium request wasn’t able to obtain any response in the time frame set up, \n",
    "            it's a sign of getting blocked, and it will raise a TimeoutException, in case of an\n",
    "            exception we simply return the same get_selenium_res function'''\n",
    "        \n",
    "        except(TimeoutException,WebDriverException):\n",
    "            # Logging the error and counting selenium retries \n",
    "            self.logger.error(traceback.format_exc())\n",
    "            self.selenium_retries += 1\n",
    "            self.logger.info('Selenium retry #: '+ str(self.selenium_retries))\n",
    "            return self.get_selenium_res(class_name)\n",
    "        \n",
    "        # If no exception is raised this code will run and returns the pages html dom.\n",
    "        else:\n",
    "            browser.maximize_window()\n",
    "            page_html = browser.page_source\n",
    "            browser.close()\n",
    "            # If succeeded the log will print success\n",
    "            self.logger.info('Success')\n",
    "            return page_html\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the csv file containing the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Getting the file and creating af panda data frame \n",
    "vivino=pd.read_csv('vivino_v7.csv',index_col=[0])\n",
    "\n",
    "#Drop duplicates by link                \n",
    "vivino=vivino\\\n",
    "            .drop_duplicates(['link'])\\\n",
    "            .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping through all links \n",
    "\n",
    "The last part of the loop where the rest of information is contained in a script tag, we need to use regex to find the information, since there's no attributes. We tried to convert the script to a json, but we weren't able to do so. Examining the script led to finding some patterns, that we could use to find and take the information needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeecda95d537446eaec9e565e1ebb723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=180.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "B=[]\n",
    "\n",
    "for a in tqdm(vivino['link'][0:180]):\n",
    "    A=[]\n",
    "    \n",
    "    '''Creating a soup with BeautifulSoup an by calling the class request with the function get_selenium_res.\n",
    "    Class name is optinal and passed as a string'''\n",
    "\n",
    "    soup = BeautifulSoup(Request(a).get_selenium_res('vivinoRatingWide__averageValue--1zL_5'), 'lxml')\n",
    "    \n",
    " # Retrieving the average price\n",
    "    p=soup.find_all('span', {'class': 'purchaseAvailabilityPPC__amount--2_4GT'})\n",
    "    # if the list p is empty, there might not be an average price, but a current price. \n",
    "    if not p:\n",
    "        g=soup.find_all('span', {'class': 'purchaseAvailability__currentPrice--3mO4u'})\n",
    "        # if g is empty, there's none of the above, and a np.nan will get pended. \n",
    "        if not g:\n",
    "            A.append(np.nan)\n",
    "        else:\n",
    "            A.append(soup.find_all('span', {'class': 'purchaseAvailability__currentPrice--3mO4u'})[0].text)\n",
    "    else:\n",
    "        A.append(soup.find_all('span', {'class': 'purchaseAvailabilityPPC__amount--2_4GT'})[0].text)\n",
    "    \n",
    " # Retrieving total number of ratings\n",
    "    ratings=soup.find_all('div', {'class': 'vivinoRatingWide__basedOn--s6y0t'})[0].text\n",
    "    # If list empty append nan\n",
    "    if not ratings:\n",
    "        A.append(np.nan)\n",
    "    # else we remove anything that's not a digit in ratings and are left with a number \n",
    "    else: \n",
    "        A.append(re.sub(r'[^\\d]','',ratings))\n",
    "    \n",
    " # Retrieving total number of reviews \n",
    "    reviews=soup.find_all('div', {'class': 'tasteCharacteristics__averageOfReviews--kut7x'})\n",
    "    # If list empty append nan\n",
    "    if not reviews:\n",
    "        A.append(np.nan)\n",
    "    # Else search for the word 'på', since this is always in front of the number needed\n",
    "    else:\n",
    "        r=str(reviews[0].text)\n",
    "        # Using the span to find placement of 'på' in string\n",
    "        n=re.search(r'(på)',r).span(0)[1]\n",
    "        # only keeping anything after 'på'\n",
    "        m=r[n:]\n",
    "        # Append the number by searching for any character before a letter\n",
    "        A.append(re.search(r'^(.+?)[a-zA-Z]', m)[1])\n",
    "        \n",
    "    \n",
    " # Retrieving the tasteprofiles\n",
    "    bar = soup.find_all('span', {'class': 'indicatorBar__progress--3aXLX'})\n",
    "    \n",
    "    # If bar is empty append nan in all 4 variables\n",
    "    if not bar:\n",
    "        for i in range(0,4):\n",
    "            A.append(np.nan)\n",
    "    #Else remove anything thats not a digit or a '.' from the 4 span tags with the specific class name\n",
    "    else:\n",
    "        for i in range(0,4):\n",
    "            A.append(re.sub(r'[^\\d\\.]','',bar[i]['style'][18:]))\n",
    "\n",
    "''' The rest of the variables can all be found in a script tag '''\n",
    "    script = str(soup.find_all(\"script\"))\n",
    "    \n",
    " # Retrieving alcohol percent \n",
    "    n=[]\n",
    "    # Searching for all groups of \"alcohol\":, and looping through the matches\n",
    "    # The script contains usually two of these, and it's the second one we need\n",
    "    for match in re.finditer(r'(\"alcohol\":)',script):\n",
    "        n.append(match.span(0))\n",
    "    span=len(n) # The total number of matches\n",
    "    \n",
    "    # if theres 0 or 1 match, no alcohol% is declared \n",
    "    if span==0 or span==1:\n",
    "        A.append(np.nan)\n",
    "    '''If there's more than one match, we find the destination of the last character in the \n",
    "    construction, \"alcohol\": , from the second match'''\n",
    "    else:\n",
    "        m=script[n[span-1][span-1]:]\n",
    "        # Some turned out to not be the right alc.pct, so nan was appended\n",
    "        if bool(re.match(r'(minimum)', re.search(r'^(.+?),', m)[1]))==True:\n",
    "            A.append(np.nan)\n",
    "        # If the right information is fetched, we take any digit after \"alcohol\": , until a comma character appears\n",
    "        else:\n",
    "            A.append(re.sub(r'\\}','',re.search(r'^(.+?),', m)[1]))\n",
    "\n",
    " # Retrieving sugar g/L\n",
    "    # if theres no match append nan\n",
    "    if re.search(r'(residual_sugar_grams_per_liter)',script)==None:\n",
    "        A.append(np.nan)\n",
    "    # If theres a match we find the destination in the script by using span\n",
    "    else:\n",
    "        n=re.search(r'(residual_sugar_grams_per_liter)',script).span(0)[1]\n",
    "        m=script[ n+2:]\n",
    "        # Recover the characters coming before a comma appears\n",
    "        A.append(re.search(r'^(.+?),', m)[1])\n",
    "\n",
    " # Retrieving acidity g/L \n",
    "    # Same method is used as the above\n",
    "    if re.search(r'(acidity_grams_per_liter)',script)==None:\n",
    "        A.append(np.nan)\n",
    "    else:\n",
    "        n=re.search(r'(acidity_grams_per_liter)',script).span(0)[1]\n",
    "        m=script[n+2:]\n",
    "        A.append(re.search(r'^(.+?),', m)[1])\n",
    "\n",
    " # Retrieving winery\n",
    "    # The winery is found through attributes. These are surrounded by \\n, which are removed by regex. \n",
    "    A.append(re.sub(\"(\\n)\", \"\", soup.find_all('a', {'class': 'winery'})[0].text))\n",
    "    \n",
    "    B.append(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframe\n",
    "\n",
    "The B retrieved from the loop is a list of lists, and is easily converted to a dataframe. The columns are:\n",
    "\n",
    "- price\n",
    "- total_ratings\n",
    "- light-bold\n",
    "- smooth-tannic\n",
    "- dry-sweet\n",
    "- soft-acidic\n",
    "- alcohol_pct\n",
    "- residual_sugar_g/l\n",
    "- acidity_g/l\n",
    "- winery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe\n",
    "df_new=pd.DataFrame(B, columns=['price', 'total_ratings','total_reviews','light-bold',\n",
    "                            'smooth-tannic','dry-sweet','soft-acidic','alcohol_pct',\n",
    "                            'residual_sugar_g/l','acidity_g/l','winery'])\n",
    "#df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning we were only able to loop through a few links at a time, so we had to merge dataframes regularly. \n",
    "The code has then been improved to loop through all the links we wish now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_new with the previous dataframe df_pre\n",
    "merged=pd.concat([df_pre,df_new]).reset_index(drop=True)\n",
    "#Remember to save the \n",
    "df_pre=merged\n",
    "df_pre.to_csv('Vivino_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: if two dataframes have to be merged to create a big dataframe\n",
    "\n",
    "#Load the other dataframe/dataframes\n",
    "df_loaded=pd.read_csv('Insert file name.csv', index_col=[0])\n",
    "\n",
    "# Merging two dataframes\n",
    "merged=pd.concat([df_loaded,vivino])\n",
    "\n",
    "#drop duplicates and reset index \n",
    "df3=merged.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save the merged dataframe \n",
    "df3.to_csv('Insert filename of your choice.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
